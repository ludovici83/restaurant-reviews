{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ludov\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('spanish'))\n",
    "stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will now define preprocessing functions for text. These will reduce the huge size of the vocabulary for vectorization. Each review will be converted to a vector where each component will be the tf-idf corresponding to a token in this reduced vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(document):\n",
    "    \"\"\"preprocess each review by 1)converting to lowercase,2) removing punctuation symbols and non-alphabetic characters\"\n",
    "    3) removing spanish stopwords and non-alphabetic characters\n",
    "    4) keeping only the stems of the words (\"buenos\"->\"buen\")\n",
    "    \"\"\"\n",
    "    document = document.lower()\n",
    "    document = document.translate(str.maketrans('', '', string.punctuation))\n",
    "    stopwordremoval = \" \".join(\n",
    "        [i for i in document.lower().split() if i not in stopwords and i.isalpha()])\n",
    "    processed_text = [stemmer.stem(i) for i in word_tokenize(stopwordremoval)]\n",
    "    return processed_text\n",
    "\n",
    "def tokenize_reviews(df_reviews):\n",
    "    # generating documents in a tokenized-stemmed format\n",
    "    docs = df_reviews[\"body\"].apply(preprocess)\n",
    "\n",
    "    docs = docs.tolist()\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(docs)\n",
    "    print(\"length of vocab before filtering\", len(id2word))\n",
    "\n",
    "    # vocab_list contains the vocabulary\n",
    "    vocab_list = [k for k in id2word.token2id.keys()]\n",
    "    print(\"length of vocab_list before filtering\", len(vocab_list))\n",
    "\n",
    "    id2word.filter_extremes(no_below=3, no_above=0.90)\n",
    "    print(\"length of vocab after filtering extremes\", len(id2word))\n",
    "\n",
    "    # vocab_list contains the filtered vocabulary\n",
    "    vocab_list = [k for k in id2word.token2id.keys()]\n",
    "\n",
    "    print(\"length of vocab after filtering extreme-words\", len(vocab_list))\n",
    "\n",
    "    # removing very short token stems\n",
    "    vocab_list = [x for x in vocab_list if len(x) > 2]\n",
    "    print(\"length of vocab after very short tokens\", len(vocab_list))\n",
    "\n",
    "    # keeping in docs only words in vocab_list (remove the filtered extreme words)\n",
    "    filtered_docs = []\n",
    "    for doc in docs:\n",
    "        filtered_doc = []\n",
    "        for elem in doc:\n",
    "            if elem in vocab_list:\n",
    "                filtered_doc.append(elem)\n",
    "        filtered_docs.append(filtered_doc)\n",
    "\n",
    "    reviews_tokenized = filtered_docs\n",
    "    return reviews_tokenized\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next we will read the reviews file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_reviews = os.path.join(\"resources\", \"reviews.json\")\n",
    "\n",
    "with open(path_reviews) as json_file:\n",
    "    reviews = json.load(json_file)\n",
    "\n",
    "df_reviews = pd.DataFrame(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vocab before filtering 4074\n",
      "length of vocab_list before filtering 4074\n",
      "length of vocab after filtering extremes 1737\n",
      "length of vocab after filtering extreme-words 1737\n",
      "length of vocab after very short tokens 1717\n"
     ]
    }
   ],
   "source": [
    "reviews_tokenized = tokenize_reviews(df_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before filtering the vocabulary word-stock there were 4074 vocabulary terms, with our approach (stemming,removing stopwords,etc) these have been reduced to 1717 tokens ; thus reducing the huge dimensionality of the review vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[\"review_tokens\"] = reviews_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(list_text):\n",
    "    string_text = ' '.join(list_text)\n",
    "    return string_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[\"review_tokens\"] = df_reviews[\"review_tokens\"].apply(list_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>uid</th>\n",
       "      <th>review_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Buena selección de ostras.</td>\n",
       "      <td>00000f8808a9789cfe57be5884ff1ad5c3b96580</td>\n",
       "      <td>buen seleccion ostras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tiene gran variedad de tapas a 2,50 de gran ca...</td>\n",
       "      <td>000010f29b5d65ad7c073acc31e327dc3ff9af54</td>\n",
       "      <td>gran varied tap gran calid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buen ambiente, trato excelente y jamón exquisito.</td>\n",
       "      <td>0000341606a7b258a202b225bb60bb615171fd18</td>\n",
       "      <td>buen ambient trat excelent jamon exquisit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Las pasta correcta pero es cara para las racio...</td>\n",
       "      <td>00003bb8ac6d31908a02cff8e372fd3434545d9a</td>\n",
       "      <td>past correct car racion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Si he estado unas 50 veces, NUNCA, he salido m...</td>\n",
       "      <td>0000c9cca3bca013e9fd7afcc7f7bc3312dfb917</td>\n",
       "      <td>unas vec nunc sal intent nombr eleg buen excel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>El clásico bar de tapas de toda la vida, con e...</td>\n",
       "      <td>f6aa4efe7ea814ce71c5697432375a42669a4f1f</td>\n",
       "      <td>clasic bar tap tod vid excelent materi prim la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Hoy hemos repetido y he querido compartir mi o...</td>\n",
       "      <td>617f4bbdf90ecd26b24fb2c7952b75e7d0181f8d</td>\n",
       "      <td>hoy repet quer compart opinion leid comentari ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>No quiero ni pensar, sin descuento: 35-40 euro...</td>\n",
       "      <td>76deeb2b327c2f1ff5340357092d15d540e055cc</td>\n",
       "      <td>quier pens descuent eur sal practic igual entr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Quedaron como lo que son, unos señores del pal...</td>\n",
       "      <td>13332e163f0ff8263d7836f106186b15a22d1ede</td>\n",
       "      <td>qued señor palad buen hac trabaj bien junt art...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Después casi tres horas, con la pizza en estóm...</td>\n",
       "      <td>dde2e67c6ecce7be8fd521600fbd844a7d62007c</td>\n",
       "      <td>despues casi tres hor pizz estomag calor comed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body  \\\n",
       "0                            Buena selección de ostras.   \n",
       "1     Tiene gran variedad de tapas a 2,50 de gran ca...   \n",
       "2     Buen ambiente, trato excelente y jamón exquisito.   \n",
       "3     Las pasta correcta pero es cara para las racio...   \n",
       "4     Si he estado unas 50 veces, NUNCA, he salido m...   \n",
       "...                                                 ...   \n",
       "4995  El clásico bar de tapas de toda la vida, con e...   \n",
       "4996  Hoy hemos repetido y he querido compartir mi o...   \n",
       "4997  No quiero ni pensar, sin descuento: 35-40 euro...   \n",
       "4998  Quedaron como lo que son, unos señores del pal...   \n",
       "4999  Después casi tres horas, con la pizza en estóm...   \n",
       "\n",
       "                                           uid  \\\n",
       "0     00000f8808a9789cfe57be5884ff1ad5c3b96580   \n",
       "1     000010f29b5d65ad7c073acc31e327dc3ff9af54   \n",
       "2     0000341606a7b258a202b225bb60bb615171fd18   \n",
       "3     00003bb8ac6d31908a02cff8e372fd3434545d9a   \n",
       "4     0000c9cca3bca013e9fd7afcc7f7bc3312dfb917   \n",
       "...                                        ...   \n",
       "4995  f6aa4efe7ea814ce71c5697432375a42669a4f1f   \n",
       "4996  617f4bbdf90ecd26b24fb2c7952b75e7d0181f8d   \n",
       "4997  76deeb2b327c2f1ff5340357092d15d540e055cc   \n",
       "4998  13332e163f0ff8263d7836f106186b15a22d1ede   \n",
       "4999  dde2e67c6ecce7be8fd521600fbd844a7d62007c   \n",
       "\n",
       "                                          review_tokens  \n",
       "0                                 buen seleccion ostras  \n",
       "1                            gran varied tap gran calid  \n",
       "2             buen ambient trat excelent jamon exquisit  \n",
       "3                               past correct car racion  \n",
       "4     unas vec nunc sal intent nombr eleg buen excel...  \n",
       "...                                                 ...  \n",
       "4995  clasic bar tap tod vid excelent materi prim la...  \n",
       "4996  hoy repet quer compart opinion leid comentari ...  \n",
       "4997  quier pens descuent eur sal practic igual entr...  \n",
       "4998  qued señor palad buen hac trabaj bien junt art...  \n",
       "4999  despues casi tres hor pizz estomag calor comed...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = df_reviews[\"review_tokens\"]\n",
    "X_vect = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ejempl',\n",
       " 'elabor',\n",
       " 'eleccion',\n",
       " 'eleg',\n",
       " 'elev',\n",
       " 'elig',\n",
       " 'ello',\n",
       " 'embarg',\n",
       " 'embut',\n",
       " 'empan']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[600:610]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(index1,index2):\n",
    "    \"\"\"this function takes as input the indices of two reviews,returns the similarity score between them\"\"\"\n",
    "    dot_product = np.dot(X_vect[index1],np.transpose(X_vect[index2]))\n",
    "    return round(dot_product.todense()[0,0],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given that the number of reviews is roughly 5000, comparing all these reviews one to one is a bit cumbersome computationally. Thus we will check our vectorization by computing the similarities between the first 500 reviews only, creating a table that for each pair of reviews gives their similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = []\n",
    "for i in range(len(df_reviews.head(500))):\n",
    "    for j in range(i,len(df_reviews.head(500))):\n",
    "        if i!=j:\n",
    "            d_ij = cosine_similarity(i,j)\n",
    "            row = (i,j,d_ij)\n",
    "            similarity.append(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = pd.DataFrame(similarity,columns=[\"i\",\"j\",\"similarity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us rank the table by similarity and let us see whether the most similar pairs of reviews in this sample share a likeness in their words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69485</th>\n",
       "      <td>167</td>\n",
       "      <td>181</td>\n",
       "      <td>0.851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122199</th>\n",
       "      <td>428</td>\n",
       "      <td>434</td>\n",
       "      <td>0.842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68493</th>\n",
       "      <td>164</td>\n",
       "      <td>188</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110319</th>\n",
       "      <td>329</td>\n",
       "      <td>434</td>\n",
       "      <td>0.839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19392</th>\n",
       "      <td>40</td>\n",
       "      <td>253</td>\n",
       "      <td>0.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120146</th>\n",
       "      <td>403</td>\n",
       "      <td>456</td>\n",
       "      <td>0.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106729</th>\n",
       "      <td>309</td>\n",
       "      <td>434</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13670</th>\n",
       "      <td>28</td>\n",
       "      <td>105</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52087</th>\n",
       "      <td>118</td>\n",
       "      <td>227</td>\n",
       "      <td>0.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35504</th>\n",
       "      <td>77</td>\n",
       "      <td>85</td>\n",
       "      <td>0.787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95675</th>\n",
       "      <td>258</td>\n",
       "      <td>345</td>\n",
       "      <td>0.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58735</th>\n",
       "      <td>136</td>\n",
       "      <td>188</td>\n",
       "      <td>0.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106307</th>\n",
       "      <td>307</td>\n",
       "      <td>393</td>\n",
       "      <td>0.774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52214</th>\n",
       "      <td>118</td>\n",
       "      <td>354</td>\n",
       "      <td>0.773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74053</th>\n",
       "      <td>181</td>\n",
       "      <td>206</td>\n",
       "      <td>0.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44574</th>\n",
       "      <td>99</td>\n",
       "      <td>124</td>\n",
       "      <td>0.746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122753</th>\n",
       "      <td>436</td>\n",
       "      <td>456</td>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15016</th>\n",
       "      <td>31</td>\n",
       "      <td>44</td>\n",
       "      <td>0.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87748</th>\n",
       "      <td>227</td>\n",
       "      <td>354</td>\n",
       "      <td>0.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58711</th>\n",
       "      <td>136</td>\n",
       "      <td>164</td>\n",
       "      <td>0.728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          i    j  similarity\n",
       "69485   167  181       0.851\n",
       "122199  428  434       0.842\n",
       "68493   164  188       0.840\n",
       "110319  329  434       0.839\n",
       "19392    40  253       0.836\n",
       "120146  403  456       0.832\n",
       "106729  309  434       0.828\n",
       "13670    28  105       0.822\n",
       "52087   118  227       0.789\n",
       "35504    77   85       0.787\n",
       "95675   258  345       0.784\n",
       "58735   136  188       0.776\n",
       "106307  307  393       0.774\n",
       "52214   118  354       0.773\n",
       "74053   181  206       0.756\n",
       "44574    99  124       0.746\n",
       "122753  436  456       0.745\n",
       "15016    31   44       0.742\n",
       "87748   227  354       0.739\n",
       "58711   136  164       0.728"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity.sort_values(by=\"similarity\",ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us compare for instance reviews 118 and 354 which have a cosine-similarity score of 0.773"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text :  Además tiene unas vistas fantásticas al mar.\n",
      "processed-tokenized text : ademas unas vist fantast mar\n"
     ]
    }
   ],
   "source": [
    "i = 118\n",
    "print(\"original text : \",df_reviews.body.iloc[i])\n",
    "print(\"processed-tokenized text :\",df_reviews.review_tokens.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text :  Y unas vistas fantásticas.\n",
      "processed-tokenized text : unas vist fantast\n"
     ]
    }
   ],
   "source": [
    "j= 354\n",
    "print(\"original text : \",df_reviews.body.iloc[j])\n",
    "print(\"processed-tokenized text :\",df_reviews.review_tokens.iloc[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One can clearly see that the two reviews are indeed very alike. They both mention the \"fantastic views\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text :  Se come estupendamente y a precios asequibles.\n",
      "processed-tokenized text : com estupend preci asequ\n"
     ]
    }
   ],
   "source": [
    "i= 167\n",
    "print(\"original text : \",df_reviews.body.iloc[i])\n",
    "print(\"processed-tokenized text :\",df_reviews.review_tokens.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text :  Estupendas tapas a precios asequibles.\n",
      "processed-tokenized text : estupend tap preci asequ\n"
     ]
    }
   ],
   "source": [
    "j = 181\n",
    "print(\"original text : \",df_reviews.body.iloc[j])\n",
    "print(\"processed-tokenized text :\",df_reviews.review_tokens.iloc[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This second example, for reviews i = 161 and 187 (with cosine similarity=0.852) also shows that they are indeed quite similar, both of them mentioning the \"affordable prices\" and using the root \"estupendo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43341</th>\n",
       "      <td>95</td>\n",
       "      <td>497</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43340</th>\n",
       "      <td>95</td>\n",
       "      <td>496</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43338</th>\n",
       "      <td>95</td>\n",
       "      <td>494</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43337</th>\n",
       "      <td>95</td>\n",
       "      <td>493</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124749</th>\n",
       "      <td>498</td>\n",
       "      <td>499</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          i    j  similarity\n",
       "43341    95  497         0.0\n",
       "43340    95  496         0.0\n",
       "43338    95  494         0.0\n",
       "43337    95  493         0.0\n",
       "124749  498  499         0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity.sort_values(by=\"similarity\",ascending=False).tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On the other hand let us check that reviews with similarity_score equal to zero are indeed totally unlike. In the example below for instance reviews 95 and 497 use completely different words in their description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text :  Encargamos una tarta aquí y el resultado fue espectacular.\n",
      "processed-tokenized text : encarg tart aqu result espectacul\n"
     ]
    }
   ],
   "source": [
    "i= 95\n",
    "print(\"original text : \",df_reviews.body.iloc[i])\n",
    "print(\"processed-tokenized text :\",df_reviews.review_tokens.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text :  ..solo cumple con lo de pudin.\n",
      "processed-tokenized text : sol cumpl pudin\n"
     ]
    }
   ],
   "source": [
    "j= 497\n",
    "print(\"original text : \",df_reviews.body.iloc[j])\n",
    "print(\"processed-tokenized text :\",df_reviews.review_tokens.iloc[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
